{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "This is rework for https://sparknlp.org/2021/01/03/ner_ud_kaist_glove_840B_300d_ko.html\n",
    "Modified for korean, with tokenizer and visualizer.\n",
    "\n",
    "Code may be updated in https://github.com/aria1th/Korean-sentense-NER-Notebook\n",
    "use `wget https://raw.githubusercontent.com/aria1th/Korean-sentense-NER-Notebook/main/NER.ipynb` to download this notebook.\n",
    "SYSTEM REQUIREMENTS\n",
    "\n",
    "Model requires at least 2.4GB of RAM, For safety, we will use increased spark memory of 8G."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Install PySpark and Spark NLP\n",
    "%pip install pyspark spark-nlp\n",
    "\n",
    "# Install Spark NLP Display lib\n",
    "%pip install --upgrade -q spark-nlp-display\n",
    "# Install KoNLPy\n",
    "%pip install konlpy\n",
    "%pip install tqdm\n",
    "# eunjeon should support mecab\n",
    "%pip install eunjeon --upgrade\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Tokenizer\n",
    "def get_tokenizer(option:str = 'Komoran', base='konlpy.tag', mecab_dict=''):\n",
    "    # available options = ['Mecab', 'Komoran', 'Okt', 'Hannanum', 'Kkma']\n",
    "    # dynamic import\n",
    "    available_options = ['Komoran', 'Okt', 'Hannanum', 'Kkma', 'Mecab']\n",
    "    if option not in available_options:\n",
    "        print(\"[Warn] Fallbacking to dynamic import for {option} from {base}, this may not work properly\")\n",
    "        return _get_tokenizer(option, base)\n",
    "    if option == 'Komoran':\n",
    "        from konlpy.tag import Komoran\n",
    "        return Komoran()\n",
    "    elif option == 'Okt':\n",
    "        from konlpy.tag import Okt\n",
    "        return Okt()\n",
    "    elif option == 'Hannanum':\n",
    "        from konlpy.tag import Hannanum\n",
    "        return Hannanum()\n",
    "    elif option == 'Kkma':\n",
    "        from konlpy.tag import Kkma\n",
    "        return Kkma()\n",
    "    elif option == 'Mecab':\n",
    "        from eunjeon import Mecab\n",
    "        if mecab_dict:\n",
    "            return Mecab(mecab_dict)\n",
    "        return Mecab()\n",
    "    else:\n",
    "        raise Exception(\"Unknown tokenizer option %s\" % option)\n",
    "\n",
    "def _get_tokenizer(clsname:str, base='konlpy.tag'):\n",
    "    return __import__(base, fromlist=[clsname]).__getattribute__(clsname)()\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "import sparknlp\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql import SparkSession\n",
    "from sparknlp.annotator import *\n",
    "from sparknlp.base import *\n",
    "from sparknlp.pretrained import PretrainedPipeline\n",
    "from pyspark.sql.types import StringType, IntegerType\n",
    "from pyspark import SparkContext\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "spark = sparknlp.start(memory='8G') #for safety, increase memory to 8G.\n",
    "\n",
    "print(\"Spark NLP version\", sparknlp.version())\n",
    "print(\"Apache Spark version:\", spark.version)\n",
    "\n",
    "spark"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "language = 'ko'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# List of sentences, not tokenized yet\n",
    "text_list = [\"\"\"모나리자는 레오나르도 다 빈치에 의해 그려진 어떤 여인의 초상화로, 파리의 루브르 박물관에 소장되어 있다.\"\"\"]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "documentAssembler = DocumentAssembler() \\\n",
    "    .setInputCol('text') \\\n",
    "    .setOutputCol('document')\n",
    "\n",
    "sentence_detector = SentenceDetector() \\\n",
    "    .setInputCols('document') \\\n",
    "    .setOutputCol('sentence')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setup NER, this should be done first\n",
    "ner = NerDLModel.pretrained(\"ner_kmou_glove_840B_300d\", \"ko\") \\\n",
    ".setInputCols([\"document\", \"token\", \"embeddings\"]) \\\n",
    ".setOutputCol(\"ner\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setup word segmenter\n",
    "word_segmenter = WordSegmenterModel.pretrained(\"wordseg_kaist_ud\", \"ko\")\\\n",
    ".setInputCols([\"sentence\"])\\\n",
    ".setOutputCol(\"token\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setup POS tagger, this is large model, so it takes time\n",
    "embeddings = WordEmbeddingsModel.pretrained(\"glove_840B_300\", \"xx\")\\\n",
    ".setInputCols(\"document\", \"token\") \\\n",
    ".setOutputCol(\"embeddings\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setup NER converter\n",
    "ner_converter = NerConverter() \\\n",
    "    .setInputCols(['sentence', 'token', 'ner']) \\\n",
    "    .setOutputCol('ner_chunk')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# setup pipeline\n",
    "nlp_pipeline = Pipeline(stages=[documentAssembler, sentence_detector, word_segmenter, embeddings, ner, ner_converter])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenize first.\n",
    "# Its not trivial to implement tokenizer for Pipeline stage, so we are just assuming documents are 'tokenized' before pipeline\n",
    "\n",
    "def tokenize(texts:list[str], tokenizer_type:str = 'Komoran', **kwargs):\n",
    "    _tokenizer = get_tokenizer(tokenizer_type, **kwargs)\n",
    "    _tokenized = []\n",
    "    for text in tqdm(texts):\n",
    "        _tokenized.append(' '.join(_tokenizer.morphs(text)))\n",
    "    return _tokenized\n",
    "\n",
    "tokenized = tokenize(text_list, 'Komoran')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# create spark dataframe\n",
    "df = spark.createDataFrame(tokenized, StringType()).toDF(\"text\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# run pipeline\n",
    "result = nlp_pipeline.fit(df).transform(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "#Visualize NER result\n",
    "from sparknlp_display import NerVisualizer\n",
    "\n",
    "NerVisualizer().display(\n",
    "    result = result.collect()[0],\n",
    "    label_col = 'ner_chunk',\n",
    "    document_col = 'document'\n",
    ")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
